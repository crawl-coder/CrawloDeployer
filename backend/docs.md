# CrawloDeployer 爬虫管理系统技术开发文档

## 1. 概述

### 1.1 项目背景

CrawloDeployer 爬虫管理系统是一个现代化的、可视化的分布式爬虫管理平台，基于 FastAPI + Vue3 构建，旨在为数据采集开发者、爬虫工程师以及需要处理大规模网络数据的团队提供一个高效、可扩展且易于配置的解决方案。

### 1.2 核心问题

- **部署复杂**：传统爬虫项目的部署和管理较为复杂，需要专业的知识和经验
- **调度困难**：缺乏可视化的任务调度和管理界面
- **监控不足**：缺少对爬虫任务执行状态和性能指标的实时监控
- **扩展性差**：难以实现分布式部署和水平扩展

### 1.3 系统功能

- **项目管理**：创建、编辑、删除爬虫项目，支持多种部署方式（ZIP包、Git仓库等）
- **任务管理**：创建、编辑、删除任务，支持定时调度（CRON表达式）
- **节点管理**：工作节点自动注册与心跳检测，实时监控节点状态
- **执行监控**：任务执行状态跟踪，详细的运行日志和性能指标
- **用户管理**：用户认证、权限管理
- **数据管理**：任务执行结果和日志的查看与管理

## 2. 系统架构设计

### 2.1 前端界面

- **技术选型**：Vue 3 + Element Plus + Vite
- **主要页面**：
  - **项目列表页**：展示所有爬虫项目的概览，支持创建新项目
  - **项目详情页**：展示单个项目的详细信息，包括任务列表、执行状态等
  - **任务管理页**：创建、编辑任务，配置执行参数和调度计划
  - **节点管理页**：展示所有工作节点的状态和资源信息
  - **任务执行记录页**：查看任务执行历史和详细日志
  - **用户管理页**：用户注册、登录和个人信息管理

### 2.2 后端服务

- **技术选型**：FastAPI + Celery + SQLAlchemy
- **主要功能模块**：
  - **项目管理API**：提供项目相关的增删改查接口
  - **任务管理API**：提供任务相关的增删改查接口，以及任务调度控制接口
  - **节点管理API**：提供节点状态查询和管理接口
  - **任务执行API**：提供任务执行记录查询接口
  - **用户认证API**：提供用户注册、登录、权限验证接口
  - **调度服务**：基于APScheduler的任务调度器

### 2.3 爬虫调度与执行

- **任务调度**：通过 APScheduler 进行任务调度，支持CRON表达式
- **任务执行**：通过 Celery 异步任务队列分发任务到工作节点
- **任务分发**：主节点将任务推送到Redis队列，工作节点从队列中获取并执行

### 2.4 数据存储与管理

- **关系数据**：用户信息、项目信息、任务配置等存储在 MySQL 8.0+ 中
- **任务队列**：使用 Redis 作为 Celery 的消息代理和结果后端
- **日志数据**：任务执行日志存储在文件系统中
- **临时数据**：使用 Redis 进行缓存和临时数据存储

## 3. 核心组件详解

### 3.1 项目管理模块

- **多方式部署**：支持上传ZIP包、单个脚本文件、多个文件或从Git仓库克隆
- **项目状态**：开发中、上线、下线三种状态管理
- **文件管理**：项目文件在服务器上的存储和管理
- **项目同步**：支持将项目文件同步到指定的工作节点

### 3.2 任务管理模块

- **任务配置**：支持配置任务参数、优先级、超时时间、重试次数等
- **定时调度**：通过CRON表达式实现灵活的定时任务调度
- **任务控制**：支持任务的启用/禁用操作
- **节点绑定**：支持将任务绑定到特定节点执行

### 3.3 节点管理模块

- **自动注册**：工作节点启动时自动向主节点注册
- **心跳检测**：定期检测节点在线状态
- **资源监控**：收集节点的CPU、内存、磁盘等资源信息
- **标签和能力**：支持为节点添加标签和定义能力

### 3.4 任务执行模块

- **多语言支持**：支持 Python、Shell、Node.js 等多种脚本语言
- **执行监控**：实时收集任务执行过程中的性能指标
- **日志记录**：详细的任务执行日志记录和查询

### 3.5 调度服务模块

- **任务同步**：定期从数据库同步启用的定时任务
- **节点监控**：实时监控工作节点状态
- **任务分发**：将定时任务转换为Celery任务并分发

## 4. 技术选型

- **前端框架**：Vue 3 + Element Plus + Vite
- **后端框架**：FastAPI + Celery + SQLAlchemy
- **数据库**：MySQL 8.0+
- **消息队列**：Redis 6.0+
- **任务调度**：APScheduler
- **日志管理**：Loguru + 文件系统
- **认证授权**：JWT Token

## 5. 数据库设计

### 5.1 核心数据表

- **用户表 (cp_users)**：存储用户基本信息和权限
- **项目表 (cp_projects)**：存储爬虫项目信息和配置
- **任务表 (cp_tasks)**：存储任务配置和调度信息
- **节点表 (cp_nodes)**：存储工作节点状态和资源信息
- **任务执行记录表 (cp_task_runs)**：存储任务执行历史和结果

### 5.2 关系设计

- 用户与项目：一对多关系（一个用户可以创建多个项目）
- 项目与任务：一对多关系（一个项目可以包含多个任务）
- 任务与执行记录：一对多关系（一个任务可以有多次执行记录）
- 节点与执行记录：一对多关系（一个节点可以执行多个任务）

## 6. 部署架构

### 6.1 主从架构

- **主节点 (Master)**：运行 FastAPI Web 应用和任务调度器
- **工作节点 (Worker)**：运行 Celery Worker 进程执行具体任务
- **通信机制**：通过 Redis 实现主节点与工作节点的异步通信

### 6.2 部署步骤

1. **环境准备**：安装 Python 3.8+、Node.js 16+、MySQL 8.0+、Redis 6.0+
2. **依赖安装**：安装后端和前端的依赖包
3. **配置文件**：配置数据库连接、Redis连接等环境变量
4. **数据库初始化**：运行 Alembic 迁移脚本创建数据表
5. **启动服务**：依次启动主节点、工作节点和前端应用

## 7. API 接口设计

### 7.1 认证接口

- `POST /api/v1/auth/login`：用户登录
- `POST /api/v1/auth/register`：用户注册
- `GET /api/v1/auth/me`：获取当前用户信息

### 7.2 项目管理接口

- `POST /api/v1/projects/`：创建项目
- `GET /api/v1/projects/`：获取项目列表
- `GET /api/v1/projects/{project_id}`：获取项目详情
- `PUT /api/v1/projects/{project_id}`：更新项目
- `DELETE /api/v1/projects/{project_id}`：删除项目
- `POST /api/v1/projects/{project_id}/sync`：将项目同步到指定节点
- `GET /api/v1/projects/{project_id}/files`：获取项目文件列表

### 7.3 任务管理接口

- `POST /api/v1/tasks/`：创建任务
- `GET /api/v1/tasks/`：获取任务列表
- `GET /api/v1/tasks/{task_id}`：获取任务详情
- `PUT /api/v1/tasks/{task_id}`：更新任务
- `DELETE /api/v1/tasks/{task_id}`：删除任务
- `POST /api/v1/tasks/{task_id}/toggle`：启用/禁用任务

### 7.4 节点管理接口

- `GET /api/v1/nodes/`：获取节点列表
- `GET /api/v1/nodes/{node_id}`：获取节点详情
- `POST /api/v1/nodes/heartbeat`：节点心跳上报

### 7.5 任务执行接口

- `GET /api/v1/task_runs/`：获取任务执行记录列表
- `GET /api/v1/task_runs/{run_id}`：获取任务执行记录详情
- `GET /api/v1/task_runs/task/{task_id}`：获取指定任务的执行记录

## 8. 开发规范

### 8.1 代码规范

- **后端**：遵循 PEP8 规范，使用 FastAPI 的依赖注入和 Pydantic 数据验证
- **前端**：遵循 Vue 3 和 Element Plus 的最佳实践
- **数据库**：使用 SQLAlchemy ORM，遵循 Alembic 迁移规范

### 8.2 API 设计规范

- **RESTful 风格**：遵循 RESTful API 设计原则
- **状态码**：合理使用 HTTP 状态码表示操作结果
- **错误处理**：统一的错误响应格式
- **版本控制**：API 版本通过 URL 路径控制

### 8.3 安全规范

- **认证授权**：JWT Token 认证，RBAC 权限控制
- **数据安全**：敏感信息加密存储，SQL 注入防护
- **接口安全**：输入验证，防止 XSS 和 CSRF 攻击

## 9. 总结

CrawloDeployer 爬虫管理系统通过现代化的技术架构和清晰的模块设计，为爬虫项目的管理提供了完整的解决方案。系统具有良好的可扩展性和可维护性，能够满足不同规模团队的爬虫管理需求。

通过合理的架构设计和技术选型，CrawloDeployer 实现了从项目部署、任务调度到执行监控的完整闭环，为用户提供了一站式的爬虫管理体验。

在分布式部署场景下，系统提供了项目文件集中管理和分发的功能，避免了在每台服务器上重复拉取代码的操作，大大简化了多节点部署的复杂度。